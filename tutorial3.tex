
\subsection*{Positive (semi-)definite matrices}

Consider symmetric \iMbox{A \in \mathbb{R}^{n \times n}},
i.e. \iMbox{A = A^{T}}

\iMbox{A} is positive-definite \textbf{iff} \iMbox{x^T Ax > 0} for all
\iMbox{x \neq 0}

\begin{itemize}

      \vItem
            \iMbox{A} is positive-definite \textbf{iff} all its eigenvalues are
            \textbf{strictly positive}
      \vItem
            \iMbox{A} is positive-definite =\textgreater{} all its diagonals are
            \textbf{strictly positive}
      \vItem
            \iMbox{A} is positive-definite =\textgreater{}
            \iMbox{\ds\max(A_{ii},A_{jj}) > |A_{ij}|}, i.e.~\textbf{strictly
                  larger coefficient} on the diagonals
      \vItem
            \iMbox{A} is positive-definite =\textgreater{} all
            \textbf{upper-left submatrices} are \emph{also} positive-definite
      \vItem
            \textbf{Sylvester's criterion}: \iMbox{A} is positive-definite
            \textbf{iff} all \textbf{upper-left submatrices} have strictly
            positive determinant
\end{itemize}

\hSep % ---

\iMbox{A} is positive semi-definite \textbf{iff} \iMbox{x^T Ax \geq 0}
for all \iMbox{x}

\begin{itemize}

      \vItem
            \iMbox{A} is positive semi-definite \textbf{iff} all its eigenvalues
            are \textbf{non-negative}
      \vItem
            \iMbox{A} is positive semi-definite =\textgreater{} all its
            diagonals are \textbf{non-negative}
      \vItem
            \iMbox{A} is positive semi-definite =\textgreater{}
            \iMbox{\ds\max(A_{ii},A_{jj}) \geq |A_{ij}|}, i.e.~\textbf{no
                  coefficient larger} than on the diagonals
      \vItem
            \iMbox{A} is positive semi-definite =\textgreater{} all
            \textbf{upper-left submatrices} are \emph{also} positive
            semi-definite
      \vItem
            \iMbox{A} is positive semi-definite =\textgreater{} it has a
            Cholesky Decomposition
\end{itemize}

\hSep % ---

For any \iMbox{M \in \mathbb{R}^{m \times n}}, \iMbox{MM^{T}} and
\iMbox{M^{T}M} are symmetric and \textbf{positive semi-definite}

\subsection*{Singular Value Decomposition (SVD) \& Singular Values}


\textbf{Singular Value Decomposition} of
\iMbox{A \in \mathbb{R}^{m \times n}} is \emph{any decomposition} of
the form \iMbox{A = USV^{T}}, where

\begin{itemize}

      \vItem
            Orthogonal \iMbox{U = [\mathbf{u}_{1}|\dots|\mathbf{u}_{m}] \in \mathbb{R}^{m \times m}}
            and \iMbox{V = [\mathbf{v}_{1}|\dots|\mathbf{v}_{n}] \in \mathbb{R}^{n \times n}}
      \vItem
            \iMbox{\ds S = \mathrm{diag}_{m \times n}(\sigma_{1},\dots,\sigma_{p})}
            where \iMbox{p = \min(m,n)} and
            \iMbox{\ds\sigma_{1}\geq\dots\geq\sigma_{p} \geq 0}
            \tcbbreak
      \vItem
            \iMbox{\ds\sigma_{1},\dots,\sigma_{p}} are \textbf{singular values}
            of \iMbox{A},

            \begin{itemize}

                  \vItem
                        \emph{(Positive)} \textbf{singular values} are \emph{(positive)}
                        \textbf{square-roots of eigenvalues} of \iMbox{AA^T} or
                        \iMbox{A^{T}A}
                  \vItem
                        i.e.~\iMbox{\ds\sigma_{1}^2,\dots,\sigma_{p}^2} are
                        \textbf{eigenvalues} of \iMbox{AA^T} or \iMbox{A^{T}A}
                  \vItem
                        \iMbox{\ds\lVert A \rVert_{2} = \sigma_{1}} \emph{(link to
                              \underline{matrix norms}}
            \end{itemize}
\end{itemize}

Let \iMbox{r = \mathrm{rk}(A)}, then number of strictly positive
\textbf{singular values} is \iMbox{r}

\begin{itemize}

      \vItem
            i.e. \iMbox{\ds\sigma_{1}\geq\dots\geq\sigma_{r} > 0} and
            \iMbox{\ds\sigma_{r+1}=\dots=\sigma_{p} = 0}
      \vItem
            \iMbox{A = \sum_{i = 1}^{r} \sigma_{i} \mathbf{u}_{i} \mathbf{v}_{i}^{T}}
\end{itemize}

\hSep % ---

\textbf{SVD} is \emph{similar} to \underline{spectral decomposition}, except it always exists

If \iMbox{n \leq m} then work with \iMbox{A^T A \in \mathbb{R}^{n \times n}}:
\begin{itemize}
      \vItem
            Obtain eigenvalues
            \iMbox{\ds\sigma_{1}^2\geq\dots\geq\sigma_{n}^2 \geq 0} of
            \iMbox{A^T A}
      \vItem
            Obtain \textbf{orthonormal} eigenvectors
            \iMbox{\mathbf{v}_{1},\dots,\mathbf{v}_{n} \in \mathbb{R}^{n}} of
            \iMbox{A^{T}A} (apply \textbf{normalization}
            e.g.~\textbf{Gram-Schmidt}!!!! to \textbf{eigenspaces}
            \iMbox{\ds E_{\sigma_{i}}})
      \vItem
            \iMbox{V = [\mathbf{v}_{1}|\dots|\mathbf{v}_{n}] \in \mathbb{R}^{n \times n}}
            is \underline{orthogonal} so \iMbox{V^{T} = V^{-1}}
      \vItem
            \iMbox{r = \mathrm{rk}(A) = \text{no. of strictly +ve} \ \ \sigma_{i}}
      \vItem
            Let
            \iMbox{\ds \mathbf{u}_{i} = \frac{1}{\sigma_{i}}A \mathbf{v}_{i}}
            then
            \iMbox{\mathbf{u}_{1},\dots,\mathbf{u}_{r} \in \mathbb{R}^{m}} are
            \textbf{orthonormal} \emph{(therefore linearly independent)}

            \begin{itemize}

                  \vItem
                        The \underline{orthogonal compliment} of
                        \iMbox{\ds\mathrm{span} \{ \mathbf{u}_{1},\dots,\mathbf{u}_{r} \}}
                        =\textgreater{}
                        \iMbox{\ds\mathrm{span} \{ \mathbf{u}_{1},\dots,\mathbf{u}_{r} \}^{\perp} = \mathrm{span} \{ \mathbf{u}_{r + 1},\dots,\mathbf{u}_{m} \}}

                        \begin{itemize}

                              \vItem
                                    Solve for unit-vector \iMbox{\mathbf{u}_{r+1}} s.t. it is
                                    orthogonal to \iMbox{\mathbf{u}_{1},\dots,\mathbf{u}_{r}}
                              \vItem
                                    Then solve for unit-vector \iMbox{\mathbf{u}_{r+2}} s.t. it is
                                    orthogonal to \iMbox{\mathbf{u}_{1},\dots,\mathbf{u}_{r+1}}
                              \vItem
                                    And so on\ldots{}
                        \end{itemize}
                  \vItem
                        \iMbox{U = [\mathbf{u}_{1}|\dots|\mathbf{u}_{m}] \in \mathbb{R}^{m \times m}}
                        is \underline{orthogonal} so \iMbox{U^{T} = U^{-1}}
            \end{itemize}
      \vItem
            \iMbox{\ds S = \mathrm{diag}_{m \times n}(\sigma_{1},\dots,\sigma_{n})},
            \textbf{AND DONE!!!}
\end{itemize}

If \iMbox{m<n} then let \iMbox{B = A^{T}}
\begin{itemize}

      \vItem
            apply above method to \iMbox{B} =\textgreater{}
            \iMbox{B = A^T = USV^{T}}
      \vItem
            \iMbox{A = B^T = V S^{T} U^{T}}
\end{itemize}



\subsection*{Tricks: Computing orthonormal vector-set extensions}

You have \textbf{orthonormal} vectors
\iMbox{\ds\mathbf{u}_{1},\dots,\mathbf{u}_{r} \in \mathbb{R}^{m}}
=\textgreater{} need to \textbf{extend} to \textbf{orthonormal}
vectors
\iMbox{\ds\mathbf{u}_{1},\dots,\mathbf{u}_{m} \in \mathbb{R}^{m}}

Special case =\textgreater{} two 3D vectors =\textgreater{} use
\textbf{cross-product} =\textgreater{} \iMbox{a \times b \perp a,b}

\hSep % ---

Extension via standard basis
\iMbox{\ds\mathbf{I}_{m} = [\mathbf{e}_{1}|\dots|\mathbf{e}_{m}]}
using \underline{\textbar(tweaked) GS}:

\begin{itemize}

      \vItem
            \textbf{Choose candidate vector}: just work through
            \iMbox{\mathbf{e}_{1},\dots,\mathbf{e}_{m}} sequentially starting
            from \iMbox{\mathbf{e}_{1}} =\textgreater{} denote the current
            candidate \iMbox{\mathbf{e}_{k}}
      \vItem
            \textbf{Orthogonalize}: Starting from \iMbox{j = r} going to
            \iMbox{j=m} with each iteration =\textgreater{} with current
            orthonormal vectors \iMbox{\ds\mathbf{u}_{1},\dots,\mathbf{u}_{j}}

            \begin{itemize}
                  \vItem
                        Compute
                        \iMbox{
                              \begin{aligned}
                                    \mathbf{w}_{j+1} & = \mathbf{e}_{k} - \sum_{i=1}^{j} (\mathbf{e}_{k} \cdot \mathbf{u}_{i})\mathbf{u}_{i}
                                    = \mathbf{e}_{k} - \sum_{i=1}^{j} (\mathbf{u}_{i})_{k} \mathbf{u}_{i}                                    \\ &= \mathbf{e}_{k} - U_{j}\mathbf{c}_{j}
                              \end{aligned}
                        }
                  \vItem Where \iMbox{\ds U_{j} = [\mathbf{u}_{1}|\dots|\mathbf{u}_{j}]} and
                        \iMbox{\ds \mathbf{c}_{j} =[(\mathbf{u}_{1})_{k},\dots,(\mathbf{u}_{j})_{k}]^{T}}
                  \vItem
                        \textbf{NOTE:}
                        \iMbox{\ds\mathbf{e}_{k} \cdot \mathbf{u}_{i} = (\mathbf{u}_{i})_{k}}
                        i.e. \iMbox{k}-th component of \iMbox{\mathbf{u}_{i}}
                  \vItem
                        If \iMbox{\ds\mathbf{w}_{j+1} = \mathbf{0}} then
                        \iMbox{\ds \mathbf{e}_{k} \in \mathrm{span}\{ \mathbf{u}_{1},\dots,\mathbf{u}_{j} \}}
                        =\textgreater{} discard \iMbox{\mathbf{w}_{j+1}}, choose next
                        candidate \iMbox{\ds \mathbf{e}_{k+1}}, try this step again
            \end{itemize}
      \vItem
            \textbf{Normalize}: \iMbox{\ds\mathbf{w}_{j+1} \neq \mathbf{0}} so
            compute unit vector
            \iMbox{\ds\mathbf{u}_{j+1} = \hat{\mathbf{w}}_{j+1}}
      \vItem
            \textbf{Repeat:} keep repeating the above steps, now with
            \textbf{new} orthonormal vectors
            \iMbox{\ds\mathbf{u}_{1},\dots,\mathbf{u}_{j+1}}
\end{itemize}


\subsection*{SVD Application: Principal Component Analysis (PCA)}

Assume \iMbox{\ds A_{\mathrm{uncentered}} \in \mathbb{R}^{m \times n}}
represent \textbf{\iMbox{m} samples} of \textbf{\iMbox{n}-dimensional
      data} \emph{(with \iMbox{m \geq n})}

\begin{itemize}

      \vItem
            \textbf{Data centering}: subtract mean of each column from that
            column's elements
      \vItem
            Let the \textbf{resulting matrix} be
            \iMbox{A \in \mathbb{R}^{m \times n}}, who's \textbf{columns} have
            \textbf{mean zero}
\end{itemize}

\textbf{PCA} is done on \textbf{centered} data-matrices like
\iMbox{A}:

\begin{itemize}

      \vItem
            SVD exists i.e.~\iMbox{A = USV^{T}} and \iMbox{r = \mathrm{rk}(A)}
      \vItem
            Let \iMbox{A = [\mathbf{r}_{1}; \ \dots; \ \mathbf{r}_{m}]} be rows
            \iMbox{\ds \mathbf{r}_{1},\dots,\mathbf{r}_{m} \in \mathbb{R}^{n}}
            =\textgreater{} each row \emph{corresponds to} a sample
      \vItem
            Let \iMbox{A = [\mathbf{c}_{1} | \dots | \mathbf{c}_{n}]} be columns
            \iMbox{\ds \mathbf{c}_{1},\dots,\mathbf{c}_{n} \in \mathbb{R}^{m}}
            =\textgreater{} each column \emph{corresponds to} one dimension of
            the data
\end{itemize}

Let \iMbox{\ds X_{1},\dots,X_{n}} be \textbf{random variables} where
each \iMbox{X_{i}} \emph{corresponds} to column \iMbox{\mathbf{c}_{i}}

\begin{itemize}

      \vItem
            i.e.~each \iMbox{X_{i}} \emph{corresponds} to \iMbox{i}-th component
            of data
      \vItem
            i.e.~random vector \iMbox{X = [X_{1}, \dots, X_{n}]^{T}} models the
            data \iMbox{\ds \mathbf{r}_{1},\dots,\mathbf{r}_{m}}
      \vItem
            \textbf{Co-variance matrix} of \iMbox{X} is
            \iMbox{\ds \mathrm{Cov}(A) = \frac{1}{m-1}A^{T}A} =\textgreater{}
            \iMbox{\ds (A^{T}A)_{ij} = (A^{T}A)_{ji} = \mathrm{Cov}(X_{i}, X_{j})}
\end{itemize}

\hSep % ---

\iMbox{\mathbf{v}_{1},\dots,\mathbf{v}_{r}} \emph{(columns of \iMbox{V})} are \textbf{principal axes} of \iMbox{A}

Let \iMbox{\ds\mathbfit{w} \in \mathbb{R}^{n}} be some unit-vector => let
\iMbox{\alpha_{j} = \mathbf{r}_{j} \cdot \mathbfit{w}} be the
\textbf{projection/coordinate} of sample \iMbox{\mathbf{r}_{j}} onto
\iMbox{\mathbfit{w}}

\begin{itemize}

      \vItem
            \textbf{Variance (Bessel's correction)} of
            \iMbox{\alpha_{1},\dots,\alpha_{m}} is
            \iMbox{
                  \begin{aligned}
                        \mathrm{Var}_{\mathbfit{w}} &= \frac{1}{m-1} \sum_{j} \alpha_{j}^2 = \frac{1}{m-1}\mathbfit{w}^T \left(\sum_{j} \mathbf{r}_{j}^{T} \mathbf{r}_{j} \right) \mathbfit{w} 
                        \\ &= \frac{1}{m-1}\mathbfit{w}^T {A}^T {A} \mathbfit{w}
                  \end{aligned}
            }
      \vItem
            \textbf{First (principal) axis defined} =\textgreater{}
            \iMbox{
                  \begin{aligned}
                        \mathbfit{w}_{(1)} &={\arg\max}_{\|\mathbfit{w}\|=1} \mathbfit{w}^T {A}^T {A} \mathbfit{w} 
                        \\ &= {\arg\max}_{\|\mathbfit{w}\|=1} (m-1)\mathrm{Var}_{\mathbfit{w}} = \mathbf{v}_{1}
                  \end{aligned}
            }
      \vItem
            i.e. \iMbox{\ds\mathbfit{w}_{(1)}} the direction that maximizes
            variance \iMbox{\mathrm{Var}_{\mathbfit{w}}}, i.e. maximizes
            variance of \textbf{projections on line}
            \iMbox{\mathbb{R}\mathbfit{w}_{(1)}}
\end{itemize}

\iMbox{\ds\sigma_{1} \mathbf{u}_{1},\dots,\sigma_{r} \mathbf{u}_{r}}
\emph{(columns of \iMbox{US})} are \textbf{principal
      components/scores} of \iMbox{A}

\begin{itemize}

      \vItem
            Recall:
            \iMbox{A = \sum_{i = 1}^{r} \sigma_{i} \mathbf{u}_{i} \mathbf{v}_{i}^{T}}
            with \iMbox{\ds\sigma_{1}\geq\dots\geq\sigma_{r} > 0}, so that
            relates principal axes and principal components
      \vItem
            \textbf{Data compression}: If \iMbox{\ds \sigma_{1} \gg \sigma_{2}}
            then \textbf{compress} \iMbox{A} by projecting in direction of
            principal component =\textgreater{}
            \iMbox{\ds A \approx \sigma_{1} \mathbf{u}_{1} \mathbf{v}_{1}^{T}}
\end{itemize}
